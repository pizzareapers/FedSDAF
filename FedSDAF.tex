\documentclass[lettersize,journal]{IEEEtran}\n\n% Packages from name.tex (IEEE template)\n\usepackage{amsmath,amsfonts}\n\usepackage{algorithmic}\n\usepackage{algorithm}\n\usepackage{array}\n\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}\n\usepackage{textcomp}\n\usepackage{stfloats}\n\usepackage{url}\n\usepackage{verbatim}\n\usepackage{graphicx}\n\usepackage{cite}\n\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}\n\n% Additional packages from arxiv.tex that are compatible and necessary\n\usepackage[utf8]{inputenc} % allow utf-8 input\n\usepackage[T1]{fontenc}    % use 8-bit T1 fonts\n\usepackage{booktabs}       % professional-quality tables\n\usepackage{nicefrac}       % compact symbols for 1/2, etc.\n\usepackage{microtype}      % microtypography\n\usepackage{amssymb} \n\usepackage{multirow}\n\usepackage{newfloat}\n\usepackage{listings}\n\usepackage{siunitx} % Added to support 'S' column type in tables\n\usepackage{colortbl}       % For cell colors in tables\n\usepackage{xcolor}         % For color definitions\n\usepackage{float}\n\usepackage{etoolbox}\n% hyperref should be loaded last, and IEEEtran recommends specific options\n\usepackage[colorlinks=true, allcolors=blue]{hyperref}\n\n\graphicspath{ {./images/} }\n\sisetup{\n  table-format = 2.2,\n  table-space-text-post = \%,\n  table-align-text-post = false\n}\n\n\begin{document}\n\n\title{FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization}\n\n\author{\n    Hongze Li\textsuperscript{*}, Zesheng Zhou\textsuperscript{*}, Zhenbiao Cao\textsuperscript{*}, Xinhui Li, Wei Chen, and Xiaojin Zhang\textsuperscript{\textdagger}\n    \thanks{*The authors contributed equally to this research.}\n    \thanks{\textdagger Corresponding author.}\n    \thanks{Hongze Li is with the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China (e-mail: lhz13012608937@outlook.com).}\n    \thanks{Zesheng Zhou is with the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China (e-mail: zhouzs@tiangong.edu.cn).}\n    \thanks{Zhenbiao Cao is with the School of Software Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China (e-mail: m202477011@hust.edu.cn).}\n    \thanks{Xinhui Li is with the School of Computer Science and Technology, Tiangong University, Tianjin, China (e-mail: lixinhui@tiangong.edu.cn).}\n    \thanks{Wei Chen is with the School of Software Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China (e-mail: lemuria\_chen@hust.edu.cn).}\n    \thanks{Xiaojin Zhang is with the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China (e-mail: xiaojinzhang@hust.edu.cn).}\n}\n\% The paper headers\n\markboth{IEEE Transactions on Multimedia, ~Vol.~XX, No.~X, OCTOBER~2025}%\n{Li \MakeLowercase{\textit{et al.}}: FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization}\n\n\maketitle\n\n\begin{abstract}\nTraditional Federated Domain Generalization (FedDG) methods focus on learning domain-invariant features or adapting to unseen target domains, often overlooking the unique knowledge embedded within the source domain, especially in strictly isolated federated learning environments. Through experimentation, we discovered a counterintuitive phenomenon: features learned from a complete source domain have superior generalization capabilities compared to those learned directly from the target domain. This insight leads us to propose the Federated Source Domain Awareness Framework (FedSDAF), the first systematic approach to enhance FedDG by leveraging source domain-aware features. FedSDAF employs a dual-adapter architecture that decouples "local expertise" from "global generalization consensus." A Domain-Aware Adapter, retained locally, extracts and protects the unique discriminative knowledge of each source domain, while a Domain-Invariant Adapter, shared across clients, builds a robust global consensus. To enable knowledge exchange, we introduce a Bidirectional Knowledge Distillation mechanism that facilitates efficient dialogue between the adapters. Extensive experiments on four benchmark datasets (OfficeHome, PACS, VLCS, and DomainNet) show that FedSDAF significantly outperforms existing FedDG methods. The source code is available at \url{https://github.com/pizzareapers/FedSDAF}.\n\end{abstract}\n\n\begin{IEEEkeywords}\nFederated Learning, Domain Generalization, Parameter-Efficient Fine-Tuning, Knowledge Distillation, Computer Vision.\n\end{IEEEkeywords}\n\n\section{Introduction}\n\begin{figure}[htbp]\n  \centering\n  \includegraphics[width=\columnwidth]{HeadPicture.pdf}\n  \caption{Illustration of domain generalization.}\n  \label{fig:head_picture}\n\end{figure}\n\IEEEPARstart{T}{he} synergy between federated learning (FL) and large-scale pre-trained models \cite{houlsby2019parameterefficienttransferlearningnlp,su2024federatedadaptiveprompttuning} has ushered in a new era of distributed learning paradigms for privacy-sensitive applications. While foundational architectures, particularly Transformers, demonstrate remarkable cross-task adaptability \cite{mahla2025exploringgradientsubspacesaddressing,chen2022fedtunedeepdiveefficient} through parameter-efficient techniques, conventional FL frameworks \cite{McMahan2016CommunicationEfficientLO,10.5555/3495724.3496362} encounter a fundamental limitation: their underlying independent and identically distributed (i.i.d.) assumption stands in stark contrast to the inherent multi-source domain heterogeneity in real-world scenarios \cite{Sahu2018FederatedOI,Peng2020Federated,9577482}. This misalignment presents a dual challenge: not only do client-specific data emerge from distinct domain distributions, but models must also simultaneously address inter-client domain shifts while maintaining robust generalization capabilities for unseen target domains during deployment. These intertwined challenges constitute the core problem in Federated Domain Generalization (FedDG).\n\nStandard domain generalization (DG) methods have advanced through domain-invariant representation learning \cite{10658240,10655267} and feature disentanglement \cite{10655401}. However, these studies primarily focus on leveraging domain-aware features in the target domain, without fully exploring the potential value of domain-aware features in the source domain.\n\nThe core challenge in FedDG is the strict data isolation among clients, which impedes knowledge alignment from diverse source domains \cite{Liu2021FedDGFD}. While existing methods exchange fragmentary information like spectral data or class prototypes \cite{Liu2021FedDGFD, 10203389, 10030422}, these incomplete representations lead to suboptimal fusion during model aggregation.\n\nHowever, our motivational study reveals a counter-intuitive finding that challenges the conventional focus on domain invariance: in isolated settings, source domain-aware features possess superior generalization capabilities. For instance, on the PACS dataset, an adapter trained solely on the "art painting" source domain achieved 97.07\% accuracy on the unseen "sketch" target, significantly outperforming an adapter trained and tested on "sketch" itself (94.94\%) \cite{8237853, 6751316}. The underlying logic is that a source-aware model, forced to bridge a significant domain gap, must discard superficial source styles (e.g., brushstrokes) and instead learn abstract, essential semantic features, which inherently generalize better to any unseen domain.\n\nThis insight reframes the problem: the true challenge is not to eliminate domain differences but to systematically leverage the unique knowledge from each source domain. This creates a core contradiction between preserving client-specific "local expertise" and building a unified "global consensus," a task where simplistic aggregation methods like FedAvg inevitably fail by diluting critical local knowledge.\n\nTo resolve this tension, we propose the Federated Source Domain-Aware Framework (FedSDAF). The innovative core of FedSDAF is a unique dual-adapter architecture that transforms this contradiction into a symbiotic relationship through principled decoupling and collaboration. We introduce a fully private, locally-retained Domain-Aware Adapter to act as a "local expert," tasked with mining and safeguarding unique source domain knowledge. In parallel, a shared Domain-Invariant Adapter serves as a "global generalizer," building a robust consensus across all clients. These two roles are interconnected by a novel Bidirectional Knowledge Distillation (BKD) mechanism. This BKD process enables a synergistic dialogue: the local adapter enhances the global model's generalization via its distilled knowledge, while the global model in turn guides the local adapter to prevent overfitting. It is this dynamic interplay between local expertise and global consensus that allows FedSDAF to systematically transform source-specific knowledge from overlooked "noise" into a core "signal" that fundamentally enhances generalization under data isolation.\n\nIn this way, FedSDAF transforms source domain knowledge from the "noise" traditionally overlooked by existing methods into a core "signal" that enhances generalization capabilities, systematically addressing the challenges posed by data isolation. Our contributions include: (1) revealing that source domain-aware features have superior generalization in federated settings and proposing FedSDAF to systematically leverage this insight; (2) designing a dual-adapter architecture with bidirectional knowledge distillation for dynamic collaborative learning; and (3) achieving state-of-the-art performance on four benchmark datasets, demonstrating the effectiveness of our approach.\n\n\section{Related Work}\n\subsection{Federated Learning and Domain Generalization}\nFederated Learning (FL) enables collaborative model training on decentralized data without sharing raw information \cite{Tang2023FPPFLFP, Fu2024FedCAFEFC, 10577749, Cai2024TowardsEF}. A primary challenge in FL is the non-Independent and Identically Distributed (non-IID) nature of client data, which impairs model performance \cite{Islam2024FedClustOF, feng2024federated}. While existing methods address this via personalization \cite{Zhang2022FedALAAL} or robust aggregation \cite{NEURIPS2024_29021b06, Li2024MaskedRN}, they often struggle to generalize to entirely unseen distributions \cite{Yuan2021WhatDW, Jiang2024HeterogeneityAwareFD}, a core problem addressed by Domain Generalization (DG).\n\nStandard DG learns robust representations from multiple source domains, often via Domain-Invariant Representation Learning (DIRL) \cite{Xu2022DIRLDR, 9879527, Liu2024FedBCGDCA, 10.1609/aaai.v38i13.29431} or Feature Disentanglement \cite{Gholami2023LatentFD}. These approaches, however, predominantly focus on leveraging features from the \textit{target} domain \cite{feng-etal-2024-two, Li2024DAAdaLD}, leaving the potential of \textit{source-domain-aware} features largely unexplored, especially in federated settings \cite{Zhang2023AggregationOD}.\n\n\subsection{Federated Domain Generalization}\nFederated Domain Generalization (FedDG) aims to enhance model generalization to unseen target domains under data isolation constraints \cite{Zhang2023FederatedDG, Radwan2024FedPartWholeFD}. Prior FedDG studies have explored sharing frequency-space information, class prototypes, or pixel-level statistics \cite{Liu2021FedDGFD, Wan2024FederatedGL, 10030422}. However, these methods often exchange shallow feature components, failing to capture deeper semantic correlations necessary for robust generalization. Our work addresses this gap by systematically exploiting source domain-aware features.\n\n\section{Methodology}\n\subsection{Problem Setup}\nIn the \textbf{Federated Domain Generalization (FedDG)} setting, we consider a scenario involving \( K \) independent clients. Each client, \( C_k \), exclusively holds a local source dataset \( D_k = \{(x_{ik}, y_{ik})\}_{i=1}^{n_k} \) that is drawn from a unique probability distribution \( P_k(X,Y) \), such that \( P_k \neq P_j \) for any \( k \neq j \). The foundational constraint of this setting is \textbf{Data Isolation}: raw data sharing among clients is strictly prohibited due to privacy concerns. Clients can only collaborate to train a model by exchanging non-sensitive information, such as model parameters or updates, via a central server. The primary objective in FedDG is to leverage the diverse, isolated source domains \( \{D_k\}_{k=1}^K \) to learn a single, robust global model \( f_{\theta_g} \) that performs well on an entirely unseen target domain \( D_t \). This target domain is drawn from a different distribution \( P_t \), where \( P_t \neq P_k \) for all source clients \( k \in \{1, \ldots, K\} \). The global model \( f_{\theta_g} \) is produced by aggregating the local models trained on each client's private data. This constrained optimization problem is formally expressed as minimizing the expected risk on the target domain:\n\[\n\theta_g^* = \min_{\theta_g} \mathbb{E}_{(x,y) \sim P_t} \left[ L(f_{\theta_g}(x), y) \right]\n\]where \( L \) represents a predefined loss function.\n\subsection{Motivational Study: The Case for Source-Domain Awareness}\nThe central hypothesis of our work is that in data-isolated settings, features learned from a complete source domain offer a more generalizable foundation than features learned from the target domain itself. To validate this, we conduct a carefully designed motivational study.\n\noindent\textbf{Experimental Setup.} We employ a strict leave-one-domain-out protocol on the PACS dataset. For each source-target pair $(\mathcal{D}_S, \mathcal{D}_T)$, we train adapters on the source domain and evaluate on the target (Source-Aware), and train/test on the same target domain (Target-Aware), measuring cross-domain generalization versus in-domain performance.\n\begin{table}[htbp]\n\caption{Performance (\%) of adapters trained on a single source domain (rows) and evaluated on a single target domain (columns).}\n\label{tab:motivation}\n\centering\n\begin{tabular}{l|cccc}\n\toprule\n\textbf{Adapter Trained on} & \multicolumn{4}{c}{\textbf{Target Domain (Y)}} \\ \textbf{(X)} & Photo & Art & Cartoon & Sketch \\ \midrule\nPhoto & 96.55 & \textbf{98.92} & \textbf{98.14} & 94.21 \\\nArt Painting & \textbf{98.88} & 98.11 & \textbf{98.25} & \textbf{97.07} \\\nCartoon & \textbf{98.03} & \textbf{98.67} & 97.23 & \textbf{95.33} \\\nSketch & \textbf{97.51} & \textbf{97.93} & 94.98 & 94.94 \\ \bottomrule\n\end{tabular}\n\end{table}\n\noindent\textbf{Results and Interpretation.} Table \ref{tab:motivation} reveals that source-aware adapters (off-diagonal) consistently outperform target-aware adapters (diagonal). For instance, on Sketch, the Art Painting adapter achieves 97.07\% versus 94.94\% for the native Sketch adapter. This stems from a fundamental learning principle: target-aware adapters overfit to domain-specific styles, while source-aware adapters, forced to bridge large domain gaps, learn abstract, semantically invariant representations that generalize better.\n\subsection{Proposed Method}\nIn this section, we formally introduce our proposed Federated Source Domain-Aware Framework (FedSDAF). The approach comprises two key components: a Knowledge Integrator Adapter (KIA) and a Bidirectional Knowledge Distillation (BKD) process.\n\begin{figure}[htbp]\n  \centering\n  \includegraphics[width=\columnwidth]{Architecture.pdf}\n  \caption{FedSDAF architecture design.}\n  \label{fig:architecture}\n\end{figure}\n\subsubsection{Knowledge Integrator Adapter (KIA)}\nThe core idea of KIA is to explicitly disentangle and leverage two complementary representation components: domain-invariant features shared by all clients and domain-specific features unique to each client. To achieve this, we introduce two types of adapters into a shared backbone neural model: a domain-invariant adapter $A_{di}$ and a domain-aware adapter $A_{dw}$.\n\textbf{Domain-Invariant Adapter $A_{di}$:} The $A_{di}$ aims to capture stable and generalizable representations across all source domains. At each federated communication round $r$, the server maintains a global domain-invariant adapter $A_{di}^r$, which is distributed to all clients for local fine-tuning. Each client $k$ optimizes a local copy of this adapter, denoted as $\hat{A}_{di}^{k,r}$, on its private dataset. After local training, the server aggregates these adapters into an updated global adapter using a weighted averaging scheme, which is essential for maintaining a robust representation that can effectively generalize across various domains.\n\textbf{Domain-Aware Adapter $A_{dw}$:} Before the first round of communication, each client initializes a  $A_{dw}$, locally. Unlike the domain-invariant adapter, this adapter is updated strictly locally throughout the training process to force it to capture the domain-aware features of each domain. Its role is to capture subtle, client-specific knowledge from the local domain, thus preserving rich domain-specific information. The domain-aware adapter is crucial for enhancing the model's ability to adapt to the specific characteristics of the client's data.\n\nThe KIA, which integrates both $A_{di}$ and $A_{dw}$ (as illustrated in Fig. \ref{fig:architecture}), is inserted following the Feedforward Neural Networks (FFNs) layer of the neural network. To effectively integrate these two sets of features, we employ a Multi-Head Self-Attention (MHSA) mechanism. Let $h$ denote the intermediate representation output from the backbone model. The integrated feature representation on client $k$ is computed as follows:\n\begin{equation} \n\hat{A}_{dw}(h) = \text{MHSA}(\text{LN}[A_{dw}(h); h]) \n\end{equation}\n\begin{equation} \nh' = h + \frac{1}{2} \hat{A}_{dw}(h) + \frac{1}{2} \hat{A}_{di}(h)\n\end{equation}\nThis integration $h'$ ensures that in each round of communication, the model effectively assimilates knowledge from the source domain while leveraging the rich contextual information inherent in the domain-aware features. By utilizing MHSA, the model can dynamically weigh the importance of various features from both the source domain and the base model's outputs, facilitating the formation of more adaptive domain-aware features.\n\subsubsection{Bidirectional Knowledge Distillation (BKD)}\nThe bidirectional knowledge distillation process serves as the core mechanism within the FedSDAF framework, designed to facilitate effective knowledge sharing among various clients. This process enables the model to transfer and integrate knowledge across different source domains, thereby achieving a comprehensive utilization of knowledge from multiple sources. Specifically, the bidirectional knowledge distillation involves computing two Kullback-Leibler (KL) divergences using the following formulas:\n\begin{equation} \nL_{di}^{KL} = \text{KL}(z_{di}(x) \parallel \hat{z}_{dw}(x))\n\end{equation}\n\begin{equation} \nL_{dw}^{KL} = \text{KL}(\\hat{z}_{dw}(x) \parallel z_{di}(x))\n\end{equation}\nHere, KL denotes the Kullback-Leibler divergence, while $z_{di}$ and $\hat{z}_{dw}$ represent the prediction logits from the model injected with $A_{di}$ and $\hat{A}_{dw}$, respectively. Through this setup, bidirectional knowledge distillation allows clients to exchange knowledge, facilitating learning and adaptation across different source domains. By sharing prediction logits, the model is able to acquire additional domain information from the server pertaining to other domains. This interaction of information is crucial in compensating for knowledge gaps arising from data isolation.\n\nDuring the actual training process, we employ a distillation loss in conjunction with Cross-Entropy Loss ($L^{CE}$), introducing a weighting factor $\alpha$ to maintain a balance between the two losses. Figure \ref{fig:optimization} illustrates this process of bi-directional distillation:\n\begin{equation} \nL = L^{CE} + \alpha (L_{di}^{KL} + L_{dw}^{KL})\n\end{equation}\n\subsubsection{Optimization process}\nThe optimization process unfolds across the server and clients. On the server, the global domain-invariant adapter $A_{di}^0$ is initialized. In each communication round $r$, the server distributes the current global adapter $A_{di}^r$ to a subset of clients. Each participating client $k$ receives $A_{di}^r$ and performs local training on its dataset $\mathcal{D}_k$. During local training, the client updates its local copy of the domain-invariant adapter $\hat{A}_{di}^{k,r}$, and its private domain-aware adapter $A_{dw}^k$, using the bidirectional knowledge distillation loss described in Equation (5). After local training, each client sends its updated adapter $\hat{A}_{di}^{k,r}$ back to the server. The server then aggregates the received adapters to produce the next round's global adapter $A_{di}^{r+1}$, using standard federated averaging:\n\begin{equation}\n    A_{di}^{r+1} \leftarrow \sum_{k=1}^{K} \frac{n_k}{N} \hat{A}_{di}^{k,r}\n\end{equation}\nwhere $n_k$ is the number of samples on client $k$ and $N = \sum_k n_k$ is the total number of samples across all clients. This iterative process allows the framework to build a robust global consensus within $A_{di}$ while leveraging the rich, localized knowledge preserved within each client's $A_{dw}$.\n\nThe complete training process of FedSDAF unfolds iteratively across the server and clients, as formalized in Algorithm \ref{alg:fedsdaf}.\n\begin{algorithm}[htbp]\n\caption{Federated Domain Generation with Bidirectional Distillation (FedSDAF)}\n\label{alg:fedsdaf}\n\begin{algorithmic}[1]\n\STATE \textbf{Require:} $K$: Number of clients; $\{D_k\}_{k=1}^K$: Client datasets; $R$: Communication rounds\n\STATE \textbf{Ensure:} $\{A_{dw}^{k,R}\}_{k=1}^K$: Well-trained client models; $A_{di}^R$: Global domain-invariant adapter\n\STATEx\n\STATE \textbf{Server Execution:}\n\STATE Initialize $A_{di}^0$, $A_{dw}^{k,0} \gets$ random initialization\n\STATE Collect $\{n_k = |D_k|\}_{k=1}^K$ from clients\n\STATE $N \gets \sum_{k=1}^K n_k$\n\STATE $w_k \gets n_k / N,\ \forall k \in \{1,\ldots,K\}$  \n\FOR{$r \gets 0$ \textbf{to} $R-1$}\n  \IF{$r > 0$}\n    \STATE Collect $\{A_{di}^{k,r}\}_{k=1}^K$  \n    \STATE $A_{di}^{r+1} \gets \sum_{k=1}^K w_k \cdot A_{di}^{k,r}$\n  \ELSE\n    \STATE $A_{di}^{r+1} \gets A_{di}^0$\n  \ENDIF\n  \STATE Broadcast $A_{di}^{r+1}$\n\ENDFOR\n\STATEx\n\STATE \textbf{Client $k$ Execution (Parallel):}\n\STATE Send $n_k = |D_k|$ to server  \n\FOR{$r \gets 0$ \textbf{to} $R-1$}\n  \STATE Receive $A_{di}^r$\n  \STATE $\hat{A}_{di}^{k,r} \gets A_{di}^r$\n  \WHILE{not converged}\n    \STATE $\min_{A_{dw}^{k,r}, \hat{A}_{di}^{k,r}} L$ using Equation (5) on $D_k$\n  \ENDWHILE\n  \STATE Send $A_{di}^{k,r}$\n\ENDFOR\n\end{algorithmic}\n\end{algorithm}\n\section{Experimental Results}\nIn this section, we describe the details of our experimental setup and report on a series of experimental results to illustrate the validity of our approach.\n\subsection{Datasets and Baselines}\n\noindent\textbf{Datasets.} We evaluate our method on four standard DG benchmarks: \textbf{PACS} \cite{8237853}, \textbf{OfficeHome} \cite{8100055}, \textbf{VLCS} \cite{6751316}, and the large-scale \textbf{DomainNet} \cite{9010750}. For all datasets, we employ a leave-one-domain-out evaluation protocol, where one domain is held out as the unseen target for testing.\n\noindent\textbf{Baseline Methods.} We compare FedSDAF against 14 methods: centralized DG methods (SWAD, HCVP, Doprompt), CNN-based FedDG methods (FedSR, FedADG, CCST, ELCFS, GA), and PEFT methods (FedCLIP, PromptFL, FedAPT, FedPR, FedMaPLe, PLAN).\n\begin{table*}[t]\n\centering\n\caption{Performance Comparison on PACS, VLCS, and OfficeHome Datasets (\%)}\n\label{tab:combined}\n\setlength{\tabcolsep}{4pt} \n\small \n\begin{tabular}{@{}l S[table-format=2.2]S[table-format=2.2]S[table-format=2.2]S[table-format=2.2] S[table-format=2.2] S[table-format=2.2]S[table-format=2.2]S[table-format=2.2]S[table-format=2.2] S[table-format=2.2] S[table-format=2.2]S[table-format=2.2]S[table-format=2.2]S[table-format=2.2] S[table-format=2.2]@{}}\n\toprule\n\multirow{2}{*}{\textbf{Methods}} & \multicolumn{5}{c}{\textbf{PACS}} & \multicolumn{5}{c}{\textbf{VLCS}} & \multicolumn{5}{c}{\textbf{OfficeHome}} \\ \cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16}\n& {A} & {C} & {P} & {S} & {\textbf{Avg.}} & {C} & {L} & {V} & {S} & {\textbf{Avg.}} & {A} & {C} & {P} & {R} & {\textbf{Avg.}} \\ \midrule\n\multicolumn{16}{@{}l}{\textit{Centralized learning based Domain Generalization Methods}} \\nSWAD       & 93.23 & 85.93 & 99.18 & 82.03 & 90.44 & 98.49 & 68.36 & 75.40 & 79.49 & 79.31 & 76.26 & 68.87 & 86.74 & 87.03 & 79.73 \\nHCVP       & 93.17 & 86.89 & 99.33 & 81.30 & 90.17 & 96.32 & 66.26 & 76.40 & 81.65 & 81.08 & 81.77 & 69.76 & 88.01 & 90.62 & 82.54 \\nDoprompt   & 95.00 & 86.35 & 99.63 & 78.20 & 89.91 & 96.70 & 66.53 & 78.28 & 79.39 & 80.23 & 80.95 & 70.88 & 88.94 & 90.10 & 82.72 \\n\addlinespace[0.5em]\n\multicolumn{16}{@{}l}{\textit{CNN-based FedDG Methods (backbone: Resnet-50)}} \\nFedSR      & 88.19 & 67.45 & 95.74 & 65.92 & 79.33 & 95.16 & 65.86 & 78.51 & 73.49 & 78.26 & 69.12 & 49.69 & 72.71 & 79.12 & 67.66 \\nFedADG     & 82.93 & 65.42 & 98.09 & 65.36 & 77.95 & 95.21 & 65.76 & 76.43 & 75.96 & 78.34 & 69.31 & 48.76 & 72.89 & 79.13 & 67.52 \\nCCST       & 87.02 & 74.57 & 98.29 & 65.84 & 81.43 & 96.49 & 65.73 & 76.42 & 77.67 & 79.08 & 69.23 & 51.36 & 72.09 & 81.27 & 68.19 \\nELCFS      & 86.77 & 73.21 & 98.14 & 65.16 & 80.82 & 95.67 & 65.02 & 76.55 & 77.96 & 79.80 & 68.17 & 50.52 & 71.44 & 80.11 & 67.56 \\nGA         & 87.68 & 75.19 & 97.56 & 65.86 & 81.57 & 96.77 & 65.16 & 78.89 & 78.93 & 79.18 & 68.62 & 50.60 & 73.35 & 81.23 & 68.45 \\n\addlinespace[0.5em]\n\multicolumn{16}{@{}l}{\textit{PEFT Methods (backbone: ViT-B/16)}} \\nFedCLIP    & 96.19 & 97.91 & 99.76 & 85.85 & 94.93 & 99.93 & 66.98 & 73.28 & 87.14 & 81.83 & 78.45 & 64.77 & 73.28 & 87.84 & 79.69 \\nPromptFL   & 96.34 & 98.46 & 99.58 & 92.19 & 96.64 & 99.71 & 68.03 & 72.24 & 85.10 & 83.59 & 82.98 & 68.98 & 92.14 & 90.27 & 83.59 \\nFedAPT     & 97.15 & 99.12 & 99.69 & 92.34 & 97.08 & 99.36 & 68.18 & 81.06 & 85.98 & 83.64 & 83.96 & 71.65 & 91.93 & 90.51 & 84.51 \\nFedPR      & 98.10 & 99.02 & 99.88 & 91.11 & 97.03 & 99.36 & 68.18 & 81.06 & 85.98 & 83.64 & 84.04 & 71.63 & 92.39 & 91.34 & 84.58 \\nFedMaPLe   & 98.44 & 99.02 & 99.94 & 90.40 & 96.95 & 98.02 & 69.54 & 82.15 & 85.81 & 83.87 & 84.56 & 72.82 & 92.38 & 91.07 & 85.21 \\nPLAN       & 98.58 & 99.14 & \textbf{99.82} & 92.08 & 97.40 & \textbf{99.18} & 69.94 & 83.75 & 88.28 & 85.29 & 86.65 & 74.73 & 93.47 & 92.06 & 86.73 \\n\midrule\n\textbf{FedSDAF} & \textbf{98.88} & \textbf{99.36} & 99.28 & \textbf{96.56} & \textbf{98.52} & 98.72 & \textbf{85.09} & \textbf{87.38} & \textbf{94.33} & \textbf{91.38} & \textbf{87.15} & \textbf{76.20} & \textbf{94.38} & \textbf{92.78} & \textbf{87.63} \\n\bottomrule\n\end{tabular}\n\end{table*}\n\begin{table}[htbp]\n\centering\n\caption{Performance Comparison on DomainNet Datasets (\%)}\n\label{tab:domainnet}\n\begingroup\n\small \n\setlength{\tabcolsep}{3pt} \n\begin{tabular}{@{}l*{7}{S[table-format=2.2]}@{}}\n\toprule\n\multirow{2}{*}{\textbf{Methods}} & \multicolumn{7}{c}{\textbf{DomainNet}} \\ \cmidrule(lr){2-8}\n & {C} & {I} & {P} & {Q} & {R} & {S} & {\textbf{Avg.}} \\ \midrule\nFedCLIP    & 74.12 & 48.36 & 68.49 & 31.73 & 80.52 & 58.62 & 60.31 \\nPromptFL   & 76.53 & 51.72 & 70.86 & 34.21 & 81.68 & 68.37 & 63.90 \\nFedAPT     & 77.02 & 51.45 & 70.36 & 49.62 & 86.64 & 68.43 & 67.25 \\nFedPR      & 75.49 & 51.96 & 71.42 & 35.98 & 82.67 & 69.43 & 64.49 \\nFedMaPLe   & 78.61 & 65.23 & 71.89 & 43.46 & 86.32 & 72.46 & 69.67 \\nPLAN       & 79.51 & \textbf{66.42} & 72.11 & 48.83 & \textbf{86.72} & 72.69 & 71.05 \\n\midrule\n\textbf{FedSDAF} & \textbf{82.59} & 62.95 & \textbf{85.12} & \textbf{54.16} & 84.99 & \textbf{74.14} & \textbf{73.99} \\n\bottomrule\n\end{tabular}\n\endgroup\n\end{table}\n\subsection{Implementation Details}\n\% MODIFIED: Condensed implementation details significantly\nWe employed ViT-B16 as the backbone, conducting 200 communication rounds with 30 local epochs per round. Dataset-specific batch sizes were: 128 (PACS, OfficeHome), 64 (VLCS), and 1024 (DomainNet). We used the Scaffold optimizer with learning rate 0.001 and PyTorch's StepLR scheduler (step size 0.2, decay factor 0.1).\n\subsection{Main Results}\nTables \ref{tab:combined} and \ref{tab:domainnet} compare FedSDAF with other methods. We achieve state-of-the-art results across all four datasets. On PACS Sketch domain, we attained 96.56\% accuracy, a 4.22\% improvement over prior methods. On DomainNet's challenging Quickdraw domain, our method excelled despite unnatural image features. These results validate our framework's ability to leverage domain-aware features from source domains effectively. While centralized DG methods perform well, they compromise data privacy. Our FedSDAF framework addresses these concerns through federated learning while achieving superior performance through bidirectional knowledge distillation and effective integration of source domain-aware features.\n\subsection{Ablation Experiment}\n\begin{table}[htbp]\n  \centering\n  \caption{Ablation Study for Different Components}\n  \label{tab:components}\n  \begingroup\n  \small\n  \setlength{\tabcolsep}{5pt}\n  \begin{tabular}{ccccccc}\n    \toprule\n    BKD & MHSA & $A_{di}$ & $A_{dw}$ & PACS & OfficeHome & VLCS \\ \midrule\n    % Use amssymb for checkmark and x\n    $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 98.52 & 87.63 & 91.38 \\n    $\times$     & $\checkmark$ & $\checkmark$ & $\checkmark$ & 97.46 & 85.81 & 86.19 \\n    $\checkmark$ & $\times$     & $\checkmark$ & $\checkmark$ & 97.13 & 85.42 & 87.20 \\n    $\checkmark$ & $\times$     & $\checkmark$ & $\times$     & 95.47 & 83.60 & 83.98 \\n    $\times$     & $\times$     & $\checkmark$ & $\times$     & 95.46 & 82.17 & 83.51 \\n    $\times$     & $\times$     & $\times$     & $\times$     & 93.29 & 78.70 & 82.69 \\n    \bottomrule\n  \end{tabular}\n  \endgroup\n\end{table}\n\subsection{Convergence Analysis}\n\begin{figure}[htbp]\n  \centering\n  \includegraphics[width=\columnwidth]{Convergence.pdf}\n  \caption{Convergence analysis on different clients in domain benchmark.}\n  \label{fig:convergence}\n\end{figure}\nFigure \ref{fig:convergence} compares FedSDAF against FedAvg on PACS. FedSDAF converges significantly faster and more stably. Within 10 rounds, FedSDAF achieves a 5.8--7.2\% advantage due to the MHSA mechanism accelerating domain-specific feature extraction. By round 40, the lead widens to 9.4--13.8\%, demonstrating the effectiveness of bidirectional knowledge distillation in overcoming domain heterogeneity.\n\subsection{Hyperparameter Analysis}\n\begin{figure}[htbp]\n  \centering\n  \includegraphics[width=\columnwidth]{alpha_sensitivity.pdf}\n  \caption{Effects of $\alpha$.}\n  \label{fig:alpha_sensitivity}\n\end{figure}\nFigure \ref{fig:alpha_sensitivity} shows FedSDAF's robustness to the loss balancing coefficient $\alpha$. Accuracy exceeds 97\% when $\alpha$ ranges from 0.3 to 5, with minimal variance. Optimal performance occurs at $\alpha=1$, confirming the framework's stability without extensive tuning.\n\subsection{Communication Cost Analysis}\nThe Domain-Aware Adapter with MHSA is kept strictly local and never aggregated, eliminating communication overhead concerns. Figure~\ref{fig:comm_cost} shows FedSDAF's total cost per client per round is approximately 2.270 MB, comparable to FedCLIP (2.010 MB) and an order of magnitude smaller than FedAvg (13.650 MB), demonstrating high communication efficiency.\n\begin{figure}[htbp]\n     \centering\n     \includegraphics[width=\columnwidth]{Communication.pdf}\n     \caption{Communication cost comparison per client. FedSDAF's total cost is comparable to other adapter-based methods and significantly lower than full-model federation.}\n     \label{fig:comm_cost}\n\end{figure}\n\subsection{Further Analysis}\n\subsubsection{Fine-grained Ablation Study on Hard Domains}\nTable~\ref{tab:hard_domain_ablation} focuses on challenging domains where performance gains are most visible. On these hard domains (PACS Photo, VLCS LabelMe, OfficeHome Clipart), each component's contribution becomes significantly more pronounced, with 3--7\% gains from BKD and MHSA, underscoring their importance for robust generalization.\n\begin{table}[htbp]\n\centering\n\caption{Fine-grained ablation study on the most challenging domains. The performance improvements for each component are more significant on these domains compared to the overall average.}\n\label{tab:hard_domain_ablation}\n\footnotesize\n\setlength{\tabcolsep}{3pt}\n\begin{tabular}{ccccccc}\n\toprule\n\multicolumn{4}{c}{\textbf{Components}} & \multicolumn{3}{c}{\textbf{Accuracy (\%) on Hard Domains}} \\ \cmidrule(r){1-4} \cmidrule(l){5-7}\n\textbf{BKD} & \textbf{MHSA} & $\boldsymbol{A_{di}}$ & $\boldsymbol{A_{dw}}$ & \textbf{PACS (P)} & \textbf{VLCS (L)} & \textbf{OfficeHome (C)} \\ \midrule\n$\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{99.28} & \textbf{85.09} & \textbf{76.20} \\ \midrule\n$\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 98.53 & 82.15 & 73.45 \\n$\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & 98.21 & 81.88 & 72.93 \\n$\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & 97.55 & 79.50 & 70.10 \\n$\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & 97.98 & 80.20 & 71.30 \\n\midrule\n$\times$ & $\times$ & $\times$ & $\times$ & 95.10 & 74.30 & 68.50 \\n\bottomrule\n\end{tabular}\n\end{table}\n\subsubsection{Visualization}\n\begin{figure}[htbp]\n  \centering\n  \includegraphics[width=\columnwidth]{tsne.pdf}\n  \caption{Visualization of the target features extracted by ViT and FedSDAF.}\n  \label{fig:tsne}\n\end{figure}\nFig. \ref{fig:tsne} applies t-SNE to visualize target features with Sketch as target domain on PACS. ViT-B/16 (d) shows dispersed features with inter-class overlap. The domain-invariant adapter (a) produces elliptical clusters with overlapping boundaries, emphasizing cross-domain alignment. The domain-aware adapter (b) generates compact but misaligned clusters, focusing on source-specific features. FedSDAF (c) achieves hierarchical disentanglement with sharp cluster margins and reduced overlap, validating that source-aware refinement requires simultaneous discriminative pattern preservation and cross-domain alignment through our dual-branch architecture.\n\subsubsection{Scalability with Increasing Client Pool Size}\n\label{sec:scalability}\nFigure~\ref{fig:scalability} evaluates FedSDAF scalability on PACS by varying client pool sizes ($K=4, 20, 30, 50$). Results demonstrate robust scalability with graceful performance degradation as clients increase, showcasing resilience to decentralization and suitability for large-scale deployments.\n\begin{figure}[htbp]\n    \centering\n    \includegraphics[width=\columnwidth]{multi_client_pacs_accuracy.pdf}\n    \caption{Performance of FedSDAF on the PACS dataset with a varying number of clients ($K$). The results show a graceful and minor decrease in accuracy as the client pool size increases, demonstrating the framework's strong scalability.}\n    \label{fig:scalability}\n\end{figure}\n\subsubsection{Analysis of Adapter Fusion Strategy}\n\label{sec:fusion_strategy}\nFigure~\ref{fig:fusion_strategy} analyzes different fusion strategies on PACS. The model achieves highest performance with equal weighting ($\alpha=1$). Both weighted-sum and concatenation methods prove effective, validating that explicit disentanglement and reintegration are the primary drivers of performance.\n\begin{figure}[htbp]\n    \centering\n    \includegraphics[width=\columnwidth]{adapter_weight_pacs_accuracy.pdf}\n    \caption{Performance comparison of different adapter fusion strategies on the PACS dataset. Both the weighted-sum and concatenation methods prove effective, with a balanced weighting scheme performing optimally.}\n    \label{fig:fusion_strategy}\n\end{figure}\n\subsubsection{Ablation Study on MHSA via Dynamic Distillation Weighting}\n\label{sec:ablation_mhsa}\nFigure~\ref{fig:hyper_alpha} shows grid search results for a FedSDAF variant without MHSA using dynamic distillation weight $\alpha_t = \alpha_{\text{max}} \times (1 - e^{-t/\tau})$. The optimal configuration ($\alpha_{\text{max}}=2.0$, $\tau=100$) achieves 97.08\% average accuracy, significantly below our full FedSDAF model, validating that MHSA provides distinct architectural advantages that cannot be emulated by adjusting loss weights alone.\n\begin{figure}[htbp]\n    \centering\n    \includegraphics[width=\columnwidth]{heatmap.pdf}\n    \caption{Heatmap of average accuracy for a FedSDAF variant \textbf{without MHSA}, using a dynamic distillation weight $\alpha_t$. The analysis explores different values for $\alpha_{\text{max}}$ and $\tau$ on the PACS dataset.}\n    \label{fig:hyper_alpha}\n\end{figure}\n\section{Conclusion}\nWe introduced FedSDAF, a novel framework for federated domain generalization that leverages source-domain-aware features. Through a dual-adapter architecture and bidirectional knowledge distillation, FedSDAF systematically transforms source domain knowledge into powerful generalization signals. Extensive experiments on four benchmarks demonstrate state-of-the-art performance. While the dual-adapter architecture introduces moderate computational overhead and performance depends on pre-trained model quality, future work could explore more efficient distillation techniques and extend the paradigm to other modalities.\n\input{ref.bbl}\n\end{document}